# Лабораторная работа №2. Метрическая классификация

**Цель работы:**
В рамках лабораторной работы реализовать алгоритм классификации KNN с использованием метода окна Парзена переменной ширины, подобрать оптимальный параметр k методом скользящего контроля (LOO), а также реализовать алгоритм отбора эталонов для повышения качества классификации.

---

## 1. Теоретическая часть

В лабораторной работе были рассмотрены следующие методы:

1. **Алгоритм метрической классификации KNN (k ближайших соседей)**
   Алгоритм классифицирует объект по метке большинства среди его k ближайших соседей на обучающей выборке.

2. **Метод окна Парзена**
   Для сглаживания классификации используется окно переменной ширины h, рассчитанное через расстояние до k-го ближайшего соседа. В качестве ядра выбрано гауссово ядро:
   K(u) = 1 / sqrt(2*pi) * exp(-u^2 / 2)

3. **Алгоритм отбора эталонов**
   Метод предполагает поочередное удаление объектов из обучающей выборки, вычисление эмпирического риска (LOO) и сохранение объекта, если его удаление увеличивает ошибку.

---

## 2. Описание кода

### 2.1. Загрузка данных

Для работы использовался датасет Breast Cancer Wisconsin (Diagnostic). Данные были очищены: удалён столбец `id`, метки `diagnosis` преобразованы в числа (`M = 1`, `B = 0`).

```python
train_x, train_y = load_base()
```

### 2.2. Реализация KNN с окном Парзена

Класс MetricClasifier реализует:

* Расчёт матрицы расстояний Евклида между объектами.
* Метод окна Парзена переменной ширины.
* Выбор оптимального k методом LOO (`train_k`).
* Предсказание меток (`predict`).
* Вычисление метрик классификации (`metrics`).
* Алгоритм отбора эталонов (`standart_select`).

### 2.3. Подбор параметра k

Для подбора k используется метод Leave-One-Out (LOO), при котором каждая точка последовательно исключается из выборки, и рассчитывается эмпирический риск:

LOO-error_i = -log P(y_i | x_i)

Затем строится график среднего риска по всем k и выбирается k с минимальным средним значением.

**Пример графика эмпирического риска:**

![Средняя ошибка по k](risk_plots_mean_errors.png)

Также строятся графики ошибок LOO для лучшего, худшего и среднего k:

* Худший k: risk_plots_worst_k_errors.png
* Лучший k: risk_plots_best_k_errors.png
* Средний k: risk_plots_mid_k_errors.png

### 2.4. Алгоритм отбора эталонов

После подбора k выполняется отбор эталонов:

1. Для каждого объекта вычисляется средний LOO-риск без него.
2. Если удаление объекта уменьшает среднюю ошибку, объект исключается.
3. Иначе объект остаётся в выборке.

Это позволяет сократить обучающую выборку без потери качества классификации.

**Пример визуализации после отбора эталонов:**

*(Вставьте сюда график распределения выбранных эталонов, если строили)*

---

## 3. Сравнение моделей

| Модель                                         | Accuracy | Precision | Recall | F1-score |
| ---------------------------------------------- | -------- | --------- | ------ | -------- |
| KNN с вашим алгоритмом (без отбора эталонов)   | 0.XXXX   | 0.XXXX    | 0.XXXX | 0.XXXX   |
| Эталонная KNN (sklearn)                        | 0.XXXX   | 0.XXXX    | 0.XXXX | 0.XXXX   |
| KNN с вашим алгоритмом (после отбора эталонов) | 0.XXXX   | 0.XXXX    | 0.XXXX | 0.XXXX   |

После отбора эталонов классификатор работает быстрее и демонстрирует сопоставимое или лучшее качество по метрикам.

---

## 4. Заключение

В ходе лабораторной работы было реализовано:

* KNN с окном Парзена переменной ширины.
* Метод LOO для подбора оптимального числа соседей k.
* Алгоритм отбора эталонов.

Построенные графики эмпирического риска позволили наглядно выбрать оптимальный k, а отбор эталонов повысил эффективность работы модели без существенной потери качества.

---

## 5. Приложения

1. Код лабораторной работы — MetricClasifier.py и вспомогательные функции.
2. Графики эмпирического риска (.png).
3. Метрики классификации для всех
